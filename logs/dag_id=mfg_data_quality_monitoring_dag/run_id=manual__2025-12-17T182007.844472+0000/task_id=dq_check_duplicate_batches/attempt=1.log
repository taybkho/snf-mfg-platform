[2025-12-17T18:20:10.333+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-12-17T18:20:10.388+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches manual__2025-12-17T18:20:07.844472+00:00 [queued]>
[2025-12-17T18:20:10.402+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches manual__2025-12-17T18:20:07.844472+00:00 [queued]>
[2025-12-17T18:20:10.403+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 2
[2025-12-17T18:20:10.423+0000] {taskinstance.py:2327} INFO - Executing <Task(PythonOperator): dq_check_duplicate_batches> on 2025-12-17 18:20:07.844472+00:00
[2025-12-17T18:20:10.436+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=1929) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-12-17T18:20:10.439+0000] {standard_task_runner.py:63} INFO - Started process 1938 to run task
[2025-12-17T18:20:10.436+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'mfg_data_quality_monitoring_dag', 'dq_check_duplicate_batches', 'manual__2025-12-17T18:20:07.844472+00:00', '--job-id', '40', '--raw', '--subdir', 'DAGS_FOLDER/mfg_data_quality_monitoring_dag.py', '--cfg-path', '/tmp/tmps3hv1vtl']
[2025-12-17T18:20:10.442+0000] {standard_task_runner.py:91} INFO - Job 40: Subtask dq_check_duplicate_batches
[2025-12-17T18:20:10.543+0000] {task_command.py:426} INFO - Running <TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches manual__2025-12-17T18:20:07.844472+00:00 [running]> on host 8717822a88d5
[2025-12-17T18:20:10.717+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='data-eng' AIRFLOW_CTX_DAG_ID='mfg_data_quality_monitoring_dag' AIRFLOW_CTX_TASK_ID='dq_check_duplicate_batches' AIRFLOW_CTX_EXECUTION_DATE='2025-12-17T18:20:07.844472+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-12-17T18:20:07.844472+00:00'
[2025-12-17T18:20:10.719+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-12-17T18:20:10.746+0000] {connection.py:414} INFO - Snowflake Connector for Python Version: 3.12.0, Python Version: 3.12.2, Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.36
[2025-12-17T18:20:10.747+0000] {connection.py:1187} INFO - Connecting to GLOBAL Snowflake domain
[2025-12-17T18:20:10.748+0000] {connection.py:1268} INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2025-12-17T18:20:12.957+0000] {cursor.py:1156} INFO - Number of results in first chunk: 1
[2025-12-17T18:20:12.959+0000] {connection.py:779} INFO - closed
[2025-12-17T18:20:13.241+0000] {connection.py:785} INFO - No async queries seem to be running, deleting session
[2025-12-17T18:20:13.473+0000] {python.py:237} INFO - Done. Returned value was: [{'check_name': 'duplicate_batch_id_core_batches', 'severity': 'ERROR', 'status': 'PASS', 'details': {'table': 'MFG_ACCELERATOR.CORE.CORE_BATCHES', 'duplicate_keys': 0, 'transform_schema': 'CORE'}}]
[2025-12-17T18:20:13.475+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-12-17T18:20:13.531+0000] {taskinstance.py:1205} INFO - Marking task as SUCCESS. dag_id=mfg_data_quality_monitoring_dag, task_id=dq_check_duplicate_batches, execution_date=20251217T182007, start_date=20251217T182010, end_date=20251217T182013
[2025-12-17T18:20:13.587+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-12-17T18:20:13.639+0000] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-12-17T18:20:13.645+0000] {local_task_job_runner.py:222} INFO - ::endgroup::

[2025-12-17T11:25:47.147+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-12-17T11:25:47.194+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [queued]>
[2025-12-17T11:25:47.205+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [queued]>
[2025-12-17T11:25:47.206+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 2
[2025-12-17T11:25:47.225+0000] {taskinstance.py:2327} INFO - Executing <Task(PythonOperator): dq_check_duplicate_batches> on 2025-12-16 05:30:00+00:00
[2025-12-17T11:25:47.247+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=2666) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-12-17T11:25:47.242+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'mfg_data_quality_monitoring_dag', 'dq_check_duplicate_batches', 'scheduled__2025-12-16T05:30:00+00:00', '--job-id', '18', '--raw', '--subdir', 'DAGS_FOLDER/mfg_data_quality_monitoring_dag.py', '--cfg-path', '/tmp/tmpj5coxp_k']
[2025-12-17T11:25:47.248+0000] {standard_task_runner.py:63} INFO - Started process 2691 to run task
[2025-12-17T11:25:47.248+0000] {standard_task_runner.py:91} INFO - Job 18: Subtask dq_check_duplicate_batches
[2025-12-17T11:25:47.345+0000] {task_command.py:426} INFO - Running <TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [running]> on host ada33a2b5ec7
[2025-12-17T11:25:47.503+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='data-eng' AIRFLOW_CTX_DAG_ID='mfg_data_quality_monitoring_dag' AIRFLOW_CTX_TASK_ID='dq_check_duplicate_batches' AIRFLOW_CTX_EXECUTION_DATE='2025-12-16T05:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-12-16T05:30:00+00:00'
[2025-12-17T11:25:47.505+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-12-17T11:25:47.527+0000] {connection.py:521} INFO - Snowflake Connector for Python Version: 3.18.0, Python Version: 3.12.2, Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.36
[2025-12-17T11:25:47.528+0000] {connection.py:1464} INFO - Connecting to GLOBAL Snowflake domain
[2025-12-17T11:25:49.961+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-12-17T11:25:49.963+0000] {taskinstance.py:2890} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 460, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/mfg_data_quality_monitoring_dag.py", line 114, in dq_check_duplicate_batches
    dupes = _fetch_one(
            ^^^^^^^^^^^
  File "/opt/airflow/dags/mfg_data_quality_monitoring_dag.py", line 74, in _fetch_one
    cur.execute(sql, params or ())
  File "/home/airflow/.local/lib/python3.12/site-packages/snowflake/connector/cursor.py", line 1134, in execute
    Error.errorhandler_wrapper(self.connection, self, error_class, errvalue)
  File "/home/airflow/.local/lib/python3.12/site-packages/snowflake/connector/errors.py", line 286, in errorhandler_wrapper
    handed_over = Error.hand_to_other_handler(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/snowflake/connector/errors.py", line 341, in hand_to_other_handler
    cursor.errorhandler(connection, cursor, error_class, error_value)
  File "/home/airflow/.local/lib/python3.12/site-packages/snowflake/connector/errors.py", line 217, in default_errorhandler
    raise error_class(
snowflake.connector.errors.ProgrammingError: 002003 (42S02): 01c11c0d-0107-4c30-0013-40ff0016d88a: SQL compilation error:
Object 'MFG_ACCELERATOR.CORE.CORE_BATCHES' does not exist or not authorized.
[2025-12-17T11:25:49.979+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=mfg_data_quality_monitoring_dag, task_id=dq_check_duplicate_batches, execution_date=20251216T053000, start_date=20251217T112547, end_date=20251217T112549
[2025-12-17T11:25:49.995+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 18 for task dq_check_duplicate_batches (002003 (42S02): 01c11c0d-0107-4c30-0013-40ff0016d88a: SQL compilation error:
Object 'MFG_ACCELERATOR.CORE.CORE_BATCHES' does not exist or not authorized.; 2691)
[2025-12-17T11:25:50.037+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2025-12-17T11:25:50.066+0000] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-12-17T11:25:50.070+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2025-12-17T13:04:52.851+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-12-17T13:04:52.911+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [queued]>
[2025-12-17T13:04:52.927+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [queued]>
[2025-12-17T13:04:52.930+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 2
[2025-12-17T13:04:52.955+0000] {taskinstance.py:2327} INFO - Executing <Task(PythonOperator): dq_check_duplicate_batches> on 2025-12-16 05:30:00+00:00
[2025-12-17T13:04:52.982+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=755) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-12-17T13:04:52.977+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'mfg_data_quality_monitoring_dag', 'dq_check_duplicate_batches', 'scheduled__2025-12-16T05:30:00+00:00', '--job-id', '11', '--raw', '--subdir', 'DAGS_FOLDER/mfg_data_quality_monitoring_dag.py', '--cfg-path', '/tmp/tmp_ekb4sf1']
[2025-12-17T13:04:52.985+0000] {standard_task_runner.py:63} INFO - Started process 773 to run task
[2025-12-17T13:04:52.985+0000] {standard_task_runner.py:91} INFO - Job 11: Subtask dq_check_duplicate_batches
[2025-12-17T13:04:53.115+0000] {task_command.py:426} INFO - Running <TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [running]> on host 84fc30c39b31
[2025-12-17T13:04:53.314+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='data-eng' AIRFLOW_CTX_DAG_ID='mfg_data_quality_monitoring_dag' AIRFLOW_CTX_TASK_ID='dq_check_duplicate_batches' AIRFLOW_CTX_EXECUTION_DATE='2025-12-16T05:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-12-16T05:30:00+00:00'
[2025-12-17T13:04:53.316+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-12-17T13:04:53.347+0000] {connection.py:414} INFO - Snowflake Connector for Python Version: 3.12.0, Python Version: 3.12.2, Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.36
[2025-12-17T13:04:53.349+0000] {connection.py:1187} INFO - Connecting to GLOBAL Snowflake domain
[2025-12-17T13:04:53.351+0000] {connection.py:1268} INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2025-12-17T13:04:55.246+0000] {connection.py:779} INFO - closed
[2025-12-17T13:04:55.448+0000] {connection.py:785} INFO - No async queries seem to be running, deleting session
[2025-12-17T13:04:55.693+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-12-17T13:04:55.694+0000] {taskinstance.py:2890} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 460, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/mfg_data_quality_monitoring_dag.py", line 114, in dq_check_duplicate_batches
    dupes = _fetch_one(
            ^^^^^^^^^^^
  File "/opt/airflow/dags/mfg_data_quality_monitoring_dag.py", line 74, in _fetch_one
    cur.execute(sql, params or ())
  File "/home/airflow/.local/lib/python3.12/site-packages/snowflake/connector/cursor.py", line 1087, in execute
    Error.errorhandler_wrapper(self.connection, self, error_class, errvalue)
  File "/home/airflow/.local/lib/python3.12/site-packages/snowflake/connector/errors.py", line 290, in errorhandler_wrapper
    handed_over = Error.hand_to_other_handler(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/snowflake/connector/errors.py", line 345, in hand_to_other_handler
    cursor.errorhandler(connection, cursor, error_class, error_value)
  File "/home/airflow/.local/lib/python3.12/site-packages/snowflake/connector/errors.py", line 221, in default_errorhandler
    raise error_class(
snowflake.connector.errors.ProgrammingError: 002003 (42S02): 01c11c70-0107-4d1a-0013-40ff00173436: SQL compilation error:
Object 'MFG_ACCELERATOR.CORE.CORE_BATCHES' does not exist or not authorized.
[2025-12-17T13:04:55.726+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=mfg_data_quality_monitoring_dag, task_id=dq_check_duplicate_batches, execution_date=20251216T053000, start_date=20251217T130452, end_date=20251217T130455
[2025-12-17T13:04:55.743+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 11 for task dq_check_duplicate_batches (002003 (42S02): 01c11c70-0107-4d1a-0013-40ff00173436: SQL compilation error:
Object 'MFG_ACCELERATOR.CORE.CORE_BATCHES' does not exist or not authorized.; 773)
[2025-12-17T13:04:55.776+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2025-12-17T13:04:55.803+0000] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-12-17T13:04:55.807+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2025-12-17T13:14:01.277+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-12-17T13:14:01.324+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [queued]>
[2025-12-17T13:14:01.339+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [queued]>
[2025-12-17T13:14:01.339+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 2
[2025-12-17T13:14:01.359+0000] {taskinstance.py:2327} INFO - Executing <Task(PythonOperator): dq_check_duplicate_batches> on 2025-12-16 05:30:00+00:00
[2025-12-17T13:14:01.372+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=1631) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-12-17T13:14:01.374+0000] {standard_task_runner.py:63} INFO - Started process 1641 to run task
[2025-12-17T13:14:01.372+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'mfg_data_quality_monitoring_dag', 'dq_check_duplicate_batches', 'scheduled__2025-12-16T05:30:00+00:00', '--job-id', '38', '--raw', '--subdir', 'DAGS_FOLDER/mfg_data_quality_monitoring_dag.py', '--cfg-path', '/tmp/tmp_utg5kbz']
[2025-12-17T13:14:01.377+0000] {standard_task_runner.py:91} INFO - Job 38: Subtask dq_check_duplicate_batches
[2025-12-17T13:14:01.476+0000] {task_command.py:426} INFO - Running <TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [running]> on host 84fc30c39b31
[2025-12-17T13:14:01.640+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='data-eng' AIRFLOW_CTX_DAG_ID='mfg_data_quality_monitoring_dag' AIRFLOW_CTX_TASK_ID='dq_check_duplicate_batches' AIRFLOW_CTX_EXECUTION_DATE='2025-12-16T05:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-12-16T05:30:00+00:00'
[2025-12-17T13:14:01.642+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-12-17T13:14:01.663+0000] {connection.py:414} INFO - Snowflake Connector for Python Version: 3.12.0, Python Version: 3.12.2, Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.36
[2025-12-17T13:14:01.665+0000] {connection.py:1187} INFO - Connecting to GLOBAL Snowflake domain
[2025-12-17T13:14:01.666+0000] {connection.py:1268} INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2025-12-17T13:14:03.154+0000] {connection.py:779} INFO - closed
[2025-12-17T13:14:03.355+0000] {connection.py:785} INFO - No async queries seem to be running, deleting session
[2025-12-17T13:14:03.636+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-12-17T13:14:03.638+0000] {taskinstance.py:2890} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 460, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/mfg_data_quality_monitoring_dag.py", line 114, in dq_check_duplicate_batches
    dupes = _fetch_one(
            ^^^^^^^^^^^
  File "/opt/airflow/dags/mfg_data_quality_monitoring_dag.py", line 74, in _fetch_one
    cur.execute(sql, params or ())
  File "/home/airflow/.local/lib/python3.12/site-packages/snowflake/connector/cursor.py", line 1087, in execute
    Error.errorhandler_wrapper(self.connection, self, error_class, errvalue)
  File "/home/airflow/.local/lib/python3.12/site-packages/snowflake/connector/errors.py", line 290, in errorhandler_wrapper
    handed_over = Error.hand_to_other_handler(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/snowflake/connector/errors.py", line 345, in hand_to_other_handler
    cursor.errorhandler(connection, cursor, error_class, error_value)
  File "/home/airflow/.local/lib/python3.12/site-packages/snowflake/connector/errors.py", line 221, in default_errorhandler
    raise error_class(
snowflake.connector.errors.ProgrammingError: 002003 (42S02): 01c11c7a-0107-4d17-0013-40ff0017410a: SQL compilation error:
Object 'MFG_ACCELERATOR.CORE.CORE_BATCHES' does not exist or not authorized.
[2025-12-17T13:14:03.654+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=mfg_data_quality_monitoring_dag, task_id=dq_check_duplicate_batches, execution_date=20251216T053000, start_date=20251217T131401, end_date=20251217T131403
[2025-12-17T13:14:03.668+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 38 for task dq_check_duplicate_batches (002003 (42S02): 01c11c7a-0107-4d17-0013-40ff0017410a: SQL compilation error:
Object 'MFG_ACCELERATOR.CORE.CORE_BATCHES' does not exist or not authorized.; 1641)
[2025-12-17T13:14:03.684+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2025-12-17T13:14:03.711+0000] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-12-17T13:14:03.716+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2025-12-17T13:19:17.038+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-12-17T13:19:17.083+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [queued]>
[2025-12-17T13:19:17.095+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [queued]>
[2025-12-17T13:19:17.096+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 2
[2025-12-17T13:19:17.115+0000] {taskinstance.py:2327} INFO - Executing <Task(PythonOperator): dq_check_duplicate_batches> on 2025-12-16 05:30:00+00:00
[2025-12-17T13:19:17.130+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=1901) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-12-17T13:19:17.129+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'mfg_data_quality_monitoring_dag', 'dq_check_duplicate_batches', 'scheduled__2025-12-16T05:30:00+00:00', '--job-id', '41', '--raw', '--subdir', 'DAGS_FOLDER/mfg_data_quality_monitoring_dag.py', '--cfg-path', '/tmp/tmpeyw_3p0s']
[2025-12-17T13:19:17.133+0000] {standard_task_runner.py:63} INFO - Started process 1910 to run task
[2025-12-17T13:19:17.133+0000] {standard_task_runner.py:91} INFO - Job 41: Subtask dq_check_duplicate_batches
[2025-12-17T13:19:17.239+0000] {task_command.py:426} INFO - Running <TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [running]> on host 84fc30c39b31
[2025-12-17T13:19:17.389+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='data-eng' AIRFLOW_CTX_DAG_ID='mfg_data_quality_monitoring_dag' AIRFLOW_CTX_TASK_ID='dq_check_duplicate_batches' AIRFLOW_CTX_EXECUTION_DATE='2025-12-16T05:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-12-16T05:30:00+00:00'
[2025-12-17T13:19:17.391+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-12-17T13:19:17.412+0000] {connection.py:414} INFO - Snowflake Connector for Python Version: 3.12.0, Python Version: 3.12.2, Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.36
[2025-12-17T13:19:17.413+0000] {connection.py:1187} INFO - Connecting to GLOBAL Snowflake domain
[2025-12-17T13:19:17.414+0000] {connection.py:1268} INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2025-12-17T13:19:20.168+0000] {cursor.py:1156} INFO - Number of results in first chunk: 1
[2025-12-17T13:19:20.181+0000] {connection.py:779} INFO - closed
[2025-12-17T13:19:20.384+0000] {connection.py:785} INFO - No async queries seem to be running, deleting session
[2025-12-17T13:19:20.604+0000] {python.py:237} INFO - Done. Returned value was: [CheckResult(check_name='duplicate_batch_id_core_batches', severity='ERROR', status='PASS', details={'duplicate_keys': 0})]
[2025-12-17T13:19:20.606+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-12-17T13:19:20.643+0000] {taskinstance.py:1205} INFO - Marking task as SUCCESS. dag_id=mfg_data_quality_monitoring_dag, task_id=dq_check_duplicate_batches, execution_date=20251216T053000, start_date=20251217T131917, end_date=20251217T131920
[2025-12-17T13:19:20.704+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-12-17T13:19:20.751+0000] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-12-17T13:19:20.756+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2025-12-17T13:30:16.671+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-12-17T13:30:16.735+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [queued]>
[2025-12-17T13:30:16.752+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [queued]>
[2025-12-17T13:30:16.758+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 2
[2025-12-17T13:30:16.780+0000] {taskinstance.py:2327} INFO - Executing <Task(PythonOperator): dq_check_duplicate_batches> on 2025-12-16 05:30:00+00:00
[2025-12-17T13:30:16.796+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=2443) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-12-17T13:30:16.799+0000] {standard_task_runner.py:63} INFO - Started process 2452 to run task
[2025-12-17T13:30:16.796+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'mfg_data_quality_monitoring_dag', 'dq_check_duplicate_batches', 'scheduled__2025-12-16T05:30:00+00:00', '--job-id', '49', '--raw', '--subdir', 'DAGS_FOLDER/mfg_data_quality_monitoring_dag.py', '--cfg-path', '/tmp/tmpxrzttqk4']
[2025-12-17T13:30:16.802+0000] {standard_task_runner.py:91} INFO - Job 49: Subtask dq_check_duplicate_batches
[2025-12-17T13:30:16.943+0000] {task_command.py:426} INFO - Running <TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [running]> on host 84fc30c39b31
[2025-12-17T13:30:17.112+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='data-eng' AIRFLOW_CTX_DAG_ID='mfg_data_quality_monitoring_dag' AIRFLOW_CTX_TASK_ID='dq_check_duplicate_batches' AIRFLOW_CTX_EXECUTION_DATE='2025-12-16T05:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-12-16T05:30:00+00:00'
[2025-12-17T13:30:17.115+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-12-17T13:30:17.135+0000] {connection.py:414} INFO - Snowflake Connector for Python Version: 3.12.0, Python Version: 3.12.2, Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.36
[2025-12-17T13:30:17.136+0000] {connection.py:1187} INFO - Connecting to GLOBAL Snowflake domain
[2025-12-17T13:30:17.137+0000] {connection.py:1268} INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2025-12-17T13:30:19.752+0000] {cursor.py:1156} INFO - Number of results in first chunk: 1
[2025-12-17T13:30:19.754+0000] {connection.py:779} INFO - closed
[2025-12-17T13:30:19.986+0000] {connection.py:785} INFO - No async queries seem to be running, deleting session
[2025-12-17T13:30:20.250+0000] {python.py:237} INFO - Done. Returned value was: [CheckResult(check_name='duplicate_batch_id_core_batches', severity='ERROR', status='PASS', details={'duplicate_keys': 0})]
[2025-12-17T13:30:20.251+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-12-17T13:30:20.287+0000] {taskinstance.py:1205} INFO - Marking task as SUCCESS. dag_id=mfg_data_quality_monitoring_dag, task_id=dq_check_duplicate_batches, execution_date=20251216T053000, start_date=20251217T133016, end_date=20251217T133020
[2025-12-17T13:30:20.318+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-12-17T13:30:20.342+0000] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-12-17T13:30:20.347+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2025-12-17T13:44:59.208+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-12-17T13:44:59.264+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [queued]>
[2025-12-17T13:44:59.281+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [queued]>
[2025-12-17T13:44:59.282+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 2
[2025-12-17T13:44:59.307+0000] {taskinstance.py:2327} INFO - Executing <Task(PythonOperator): dq_check_duplicate_batches> on 2025-12-16 05:30:00+00:00
[2025-12-17T13:44:59.329+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=220) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-12-17T13:44:59.323+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'mfg_data_quality_monitoring_dag', 'dq_check_duplicate_batches', 'scheduled__2025-12-16T05:30:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/mfg_data_quality_monitoring_dag.py', '--cfg-path', '/tmp/tmp0k09juob']
[2025-12-17T13:44:59.331+0000] {standard_task_runner.py:63} INFO - Started process 232 to run task
[2025-12-17T13:44:59.331+0000] {standard_task_runner.py:91} INFO - Job 4: Subtask dq_check_duplicate_batches
[2025-12-17T13:44:59.462+0000] {task_command.py:426} INFO - Running <TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [running]> on host 8717822a88d5
[2025-12-17T13:44:59.662+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='data-eng' AIRFLOW_CTX_DAG_ID='mfg_data_quality_monitoring_dag' AIRFLOW_CTX_TASK_ID='dq_check_duplicate_batches' AIRFLOW_CTX_EXECUTION_DATE='2025-12-16T05:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-12-16T05:30:00+00:00'
[2025-12-17T13:44:59.666+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-12-17T13:44:59.695+0000] {connection.py:414} INFO - Snowflake Connector for Python Version: 3.12.0, Python Version: 3.12.2, Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.36
[2025-12-17T13:44:59.697+0000] {connection.py:1187} INFO - Connecting to GLOBAL Snowflake domain
[2025-12-17T13:44:59.698+0000] {connection.py:1268} INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2025-12-17T13:45:02.135+0000] {cursor.py:1156} INFO - Number of results in first chunk: 1
[2025-12-17T13:45:02.137+0000] {connection.py:779} INFO - closed
[2025-12-17T13:45:02.349+0000] {connection.py:785} INFO - No async queries seem to be running, deleting session
[2025-12-17T13:45:02.560+0000] {python.py:237} INFO - Done. Returned value was: [CheckResult(check_name='duplicate_batch_id_core_batches', severity='ERROR', status='PASS', details={'duplicate_keys': 0})]
[2025-12-17T13:45:02.562+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-12-17T13:45:02.612+0000] {taskinstance.py:1205} INFO - Marking task as SUCCESS. dag_id=mfg_data_quality_monitoring_dag, task_id=dq_check_duplicate_batches, execution_date=20251216T053000, start_date=20251217T134459, end_date=20251217T134502
[2025-12-17T13:45:02.660+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-12-17T13:45:02.712+0000] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-12-17T13:45:02.717+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2025-12-17T18:25:31.488+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-12-17T18:25:31.535+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [queued]>
[2025-12-17T18:25:31.545+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [queued]>
[2025-12-17T18:25:31.546+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 2
[2025-12-17T18:25:31.563+0000] {taskinstance.py:2327} INFO - Executing <Task(PythonOperator): dq_check_duplicate_batches> on 2025-12-16 05:30:00+00:00
[2025-12-17T18:25:31.576+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=2343) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-12-17T18:25:31.579+0000] {standard_task_runner.py:63} INFO - Started process 2351 to run task
[2025-12-17T18:25:31.576+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'mfg_data_quality_monitoring_dag', 'dq_check_duplicate_batches', 'scheduled__2025-12-16T05:30:00+00:00', '--job-id', '48', '--raw', '--subdir', 'DAGS_FOLDER/mfg_data_quality_monitoring_dag.py', '--cfg-path', '/tmp/tmp7utj03xa']
[2025-12-17T18:25:31.581+0000] {standard_task_runner.py:91} INFO - Job 48: Subtask dq_check_duplicate_batches
[2025-12-17T18:25:31.682+0000] {task_command.py:426} INFO - Running <TaskInstance: mfg_data_quality_monitoring_dag.dq_check_duplicate_batches scheduled__2025-12-16T05:30:00+00:00 [running]> on host 8717822a88d5
[2025-12-17T18:25:31.814+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='data-eng' AIRFLOW_CTX_DAG_ID='mfg_data_quality_monitoring_dag' AIRFLOW_CTX_TASK_ID='dq_check_duplicate_batches' AIRFLOW_CTX_EXECUTION_DATE='2025-12-16T05:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-12-16T05:30:00+00:00'
[2025-12-17T18:25:31.816+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-12-17T18:25:31.836+0000] {connection.py:414} INFO - Snowflake Connector for Python Version: 3.12.0, Python Version: 3.12.2, Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.36
[2025-12-17T18:25:31.838+0000] {connection.py:1187} INFO - Connecting to GLOBAL Snowflake domain
[2025-12-17T18:25:31.839+0000] {connection.py:1268} INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2025-12-17T18:25:33.847+0000] {cursor.py:1156} INFO - Number of results in first chunk: 1
[2025-12-17T18:25:33.849+0000] {connection.py:779} INFO - closed
[2025-12-17T18:25:34.087+0000] {connection.py:785} INFO - No async queries seem to be running, deleting session
[2025-12-17T18:25:34.323+0000] {python.py:237} INFO - Done. Returned value was: [{'check_name': 'duplicate_batch_id_core_batches', 'severity': 'ERROR', 'status': 'PASS', 'details': {'table': 'MFG_ACCELERATOR.CORE.CORE_BATCHES', 'duplicate_keys': 0, 'transform_schema': 'CORE'}}]
[2025-12-17T18:25:34.324+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-12-17T18:25:34.355+0000] {taskinstance.py:1205} INFO - Marking task as SUCCESS. dag_id=mfg_data_quality_monitoring_dag, task_id=dq_check_duplicate_batches, execution_date=20251216T053000, start_date=20251217T182531, end_date=20251217T182534
[2025-12-17T18:25:34.379+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-12-17T18:25:34.404+0000] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-12-17T18:25:34.408+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
